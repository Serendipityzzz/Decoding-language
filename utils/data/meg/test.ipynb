{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "# local dep\n",
    "if __name__ == \"__main__\":\n",
    "    import os, sys\n",
    "    sys.path.insert(0, os.path.join(os.pardir, os.pardir, os.pardir))\n",
    "from utils import DotDict\n",
    "from utils.data import load_pickle\n",
    "\n",
    "__all__ = [\n",
    "    \"load_gwilliams2022neural_origin\",\n",
    "]\n",
    "\n",
    "# def load_gwilliams2022neural_origin func\n",
    "def load_gwilliams2022neural_origin(path_data, subjects_allowed=None):\n",
    "    \"\"\"\n",
    "    Load the origin data from specified data path.\n",
    "    :param path_data: path - The path of specified data.\n",
    "    :param subjects_allowed: (n_subjects[list],) - The allowed subjects to load data.\n",
    "    :return dataset_train: (n_train[list],) - The train dataset.\n",
    "    :return dataset_test: (n_test[list],) - The test dataset.\n",
    "    \"\"\"\n",
    "    # Initialize path_subjects.\n",
    "    path_subjects = [os.path.join(path_data, path_i) for path_i in os.listdir(path_data)\\\n",
    "        if os.path.isdir(os.path.join(path_data, path_i)) and path_i.startswith(\"sub-\")]; path_subjects.sort()\n",
    "    # Initialize dataset_train & dataset_test.\n",
    "    dataset_train = []; dataset_test = []\n",
    "    subjects_allowed = subjects_allowed if subjects_allowed is not None else np.arange(len(path_subjects)).tolist()\n",
    "    for subject_id, path_subject in enumerate(path_subjects):\n",
    "        if subject_id not in subjects_allowed: continue\n",
    "        dataset_i = load_pickle(os.path.join(path_subject, \"dataset.origin\"))\n",
    "        dataset_train_i, dataset_test_i = dataset_i.train, dataset_i.test\n",
    "        for data_train_idx in range(len(dataset_train_i)):\n",
    "            dataset_train_i[data_train_idx][\"subject_id\"] = subject_id\n",
    "        for data_test_idx in range(len(dataset_test_i)):\n",
    "            dataset_test_i[data_test_idx][\"subject_id\"] = subject_id\n",
    "        dataset_train.extend(dataset_train_i); dataset_test.extend(dataset_test_i)\n",
    "    # Log information related to loaded dataset.\n",
    "    print((\n",
    "        \"INFO: Get {:d} samples in trainset, and {:d} samples in testset in utils.data.meg.gwilliams2022neural.\"\n",
    "    ).format(len(dataset_train), len(dataset_test)))\n",
    "    # Return the final `dataset_train` & `dataset_test`.\n",
    "    return dataset_train, dataset_test\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n",
      "INFO: Get 22744 samples in trainset, and 11180 samples in testset in utils.data.meg.gwilliams2022neural.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# macro\n",
    "base = os.path.join(os.getcwd(), os.pardir, os.pardir, os.pardir)\n",
    "path_data = os.path.join(base, \"data\", \"meg.gwilliams2022neural\")\n",
    "subjects_allowed = list(range(2))\n",
    "print(subjects_allowed)\n",
    "# Load data from specified data.\n",
    "data = load_gwilliams2022neural_origin(path_data, subjects_allowed=subjects_allowed)\n",
    "# print(data[0][0].keys()) # dict_keys(['name', 'audio', 'data', 'chan_pos', 'subject_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def downsample_array(array, original_sample_rate, target_sample_rate):\n",
    "    original_length = len(array)\n",
    "    target_length = int(original_length * target_sample_rate / original_sample_rate)\n",
    "    indices = np.linspace(0, original_length - 1, target_length).astype(int)\n",
    "    downsampled_array = np.interp(indices, np.arange(original_length), array)\n",
    "    return downsampled_array\n",
    "\n",
    "# Example usage\n",
    "original_sample_rate = 22050\n",
    "target_sample_rate = 16000\n",
    "\n",
    "# Generate a random array with length = original_sample_rate * time_length\n",
    "array = np.random.rand(original_sample_rate * 3)\n",
    "\n",
    "# Downsample the array\n",
    "downsampled_array = downsample_array(array, original_sample_rate, target_sample_rate)\n",
    "print(downsampled_array.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "meg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
